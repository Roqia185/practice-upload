{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maroof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Maroof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy image (3 color channels, RGB)\n",
    "dummy_image = np.ones((500, 500, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "# Draw a few shapes to simulate \"objects\"\n",
    "cv2.rectangle(dummy_image, (50, 50), (200, 200), (0, 255, 0), -1)  # Green rectangle top-left and bottom-right corners, \n",
    "cv2.circle(dummy_image, (300, 300), 50, (255, 0, 0), -1)  # Blue circle (300, 300) is the center, 50 is the radius,\n",
    "\n",
    "# Convert dummy image to tensor and normalize it for the model\n",
    "image_tensor = F.to_tensor(dummy_image).unsqueeze(0) #normalize the image pixel values 0 to 1 convert into tensor,input shape ([batch_size, channels, height, width]).\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad(): #disable the gradient calculation to speed up the prdiction\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Draw bounding boxes on the dummy image\n",
    "predicted_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "predicted_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Threshold to visualize only confident predictions\n",
    "threshold = 0.5\n",
    "\n",
    "for box, score in zip(predicted_boxes, predicted_scores):\n",
    "    if score >= threshold:\n",
    "        # Draw bounding box\n",
    "        start_point = (int(box[0]), int(box[1]))    #top-left and bottom-right coordinates of the bounding box\n",
    "        end_point = (int(box[2]), int(box[3]))  #bottom-right\n",
    "        cv2.rectangle(dummy_image, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(cv2.cvtColor(dummy_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Task 1: Understanding and Modifying Object Detection Model Loading\n",
    "Objective: Understand the purpose of loading a pre-trained model and practice loading different models and understanding their layers.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Import torch and torchvision libraries.\n",
    "Load the fasterrcnn_resnet50_fpn model with pretrained=True and set it to evaluation mode (model.eval()).\n",
    "\n",
    "Task 1.1: Print the model architecture to understand its layers. Identify and explain the function of at least three key components of the Faster R-CNN model.\n",
    "\n",
    "Task 1.2: Explore other object detection models in torchvision.models.detection. Load an alternative model (e.g., retinanet_resnet50_fpn) and print its architecture to compare it with Faster R-CNN.\n",
    "Expected Outcome:\n",
    "\n",
    "A basic understanding of the model architecture and key components in Faster R-CNN and an exploration of an alternative model.\n",
    "\n",
    "Lab Task 2: Creating and Preprocessing Dummy Data for Object Detection\n",
    "Objective: Create a synthetic image with shapes to simulate \"objects\" and preprocess the image to prepare it for the model input.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Use numpy to create a white image of size 500x500 pixels with three channels (RGB).\n",
    "Use OpenCV (cv2) to draw at least two different shapes, simulating objects on the image:\n",
    "\n",
    "Task 2.1: Draw a green rectangle and a blue circle, as shown in the original code.\n",
    "\n",
    "Task 2.2: Add an additional shape of your choice (e.g., a red triangle or yellow ellipse).\n",
    "\n",
    "Convert the created image to a PyTorch tensor using torchvision.transforms.functional.to_tensor and add a batch dimension (unsqueeze(0)).\n",
    "Expected Outcome:\n",
    "\n",
    "A dummy image with at least three distinct shapes representing \"objects.\"\n",
    "An understanding of the image-to-tensor conversion and normalization process for input into a pre-trained model.\n",
    "\n",
    "Lab Task 3: Performing Object Detection and Drawing Bounding Boxes\n",
    "Objective: Run inference on the dummy image using Faster R-CNN, filter predictions based on a confidence threshold, and visualize the bounding boxes.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Use torch.no_grad() to disable gradient computation, making inference more efficient.\n",
    "Perform inference on the image tensor using the Faster R-CNN model and extract boxes and scores from predictions.\n",
    "Task 3.1: Set a confidence threshold (e.g., 0.5) and filter boxes with scores above the threshold.\n",
    "Task 3.2: Draw bounding boxes on the original dummy image using OpenCVâ€™s cv2.rectangle() function. Display the final image using matplotlib.pyplot.\n",
    "Task 3.3: Experiment with different threshold values and observe how it affects the displayed bounding boxes. Document your findings.\n",
    "Expected Outcome:\n",
    "\n",
    "A visual output displaying bounding boxes around the detected shapes on the dummy image.\n",
    "An understanding of the importance of confidence thresholds in filtering predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
